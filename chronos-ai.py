#!/usr/bin/env python3
"""
AI Agent for 6-Station 3D Printer Production Line
Enhanced with Chronos Time-Series Forecasting for predictive analytics
"""
import warnings
import argparse
import json
import os
import sys
import time
import re
from typing import List, Dict, Any, Optional, Tuple
import numpy as np
import pandas as pd
from joblib import dump, load
from datetime import datetime, timedelta

# Chronos imports for forecasting
CHRONOS_AVAILABLE = False
CHRONOS_METHOD = 'none'

try:
    # Try official Amazon Chronos package first
    from chronos import ChronosPipeline
    import torch
    CHRONOS_AVAILABLE = True
    CHRONOS_METHOD = 'chronos_package'
    print("‚úì Chronos package available")
except ImportError as e:
    print(f"‚ö†Ô∏è  Chronos package not found: {e}")
    try:
        # Try transformers as fallback
        from transformers import pipeline
        import torch
        CHRONOS_AVAILABLE = True
        CHRONOS_METHOD = 'transformers'
        print("‚úì Transformers available (fallback method)")
    except ImportError:
        print("‚ö†Ô∏è  Neither Chronos nor Transformers available")
        print("   Using simple exponential smoothing forecasting")
        CHRONOS_AVAILABLE = False
        CHRONOS_METHOD = 'none'

warnings.filterwarnings('ignore')

# Constants
RANDOM_STATE = 42
DEFAULT_MODEL_PATH = 'line_forecaster.joblib'
DEFAULT_TEST_SIZE = 0.2
CV_FOLDS = 5
FORECAST_HORIZON = 24  # Default forecast horizon (hours)
# Options: small, base, large, tiny
CHRONOS_MODEL_NAME = "amazon/chronos-t5-small"

# Target metrics to forecast for each station
FORECAST_TARGETS = [
    'cycle_time_s',
    'queue_len',
    'defect_rate',
    'failure_rate',
    'downtime_sec',
    'throughput_per_hour',
    'utilization_pct'
]

# ==================== INTELLIGENT RECOMMENDATION SYSTEM ====================
# Recommendations based on forecasted trends (proactive instead of reactive)
INTELLIGENT_RECOMMENDATIONS = {
    # S1: Component Kitting & Pre-Assembly
    'S1': {
        'bottleneck_forecast': [
            "‚ö†Ô∏è Queue length forecast to increase by {growth_pct}% in next {horizon}h",
            "Add {additional_machines} additional collaborative robot arms before bottleneck occurs",
            "Optimize gripper changeover sequence (target {reduction_pct}% reduction)",
            "Increase S1‚ÜíS2 buffer to {buffer_target} units (currently {current_buffer})",
            "Schedule buffer expansion within {action_window} hours to prevent disruption"
        ],
        'quality_risk_forecast': [
            "‚ö†Ô∏è Defect rate forecast to exceed threshold ({forecast_value:.2%} vs {threshold:.2%})",
            "Implement barcode scanning for component verification within {action_window}h",
            "Add torque monitoring for critical fasteners",
            "Schedule preventive calibration of vision systems"
        ],
        'maintenance_forecast': [
            "‚ö†Ô∏è Failure rate trending upward - MTBF forecast: {mtbf_forecast:.1f}h (current: {mtbf_current:.1f}h)",
            "Schedule preventive maintenance for collaborative robot arms within {action_window}h",
            "Replace worn grippers and check alignment sensors",
            "Inspect cable carriers and pneumatic lines"
        ]
    },
    # S2: Frame and Core Assembly
    'S2': {
        'bottleneck_forecast': [
            "‚ö†Ô∏è Cycle time forecast to increase to {forecast_value:.1f}s (current: {current_value:.1f}s)",
            "Upgrade to high-speed bearing press (reduce cycle time by {improvement_pct}%)",
            "Add automated lubrication system to reduce failures",
            "Increase S1‚ÜíS2 buffer from {current_buffer} to {buffer_target} units"
        ],
        'quality_risk_forecast': [
            "‚ö†Ô∏è Quality metrics forecast to degrade in {horizon}h window",
            "Install laser alignment system for frame assembly",
            "Add torque verification for critical structural fasteners",
            "Implement automated bed leveling verification"
        ],
        'maintenance_forecast': [
            "‚ö†Ô∏è Predictive maintenance alert: Equipment health declining",
            "Service bearing press hydraulic system within {action_window}h",
            "Replace linear rail lubrication and check alignment",
            "Inspect frame welding fixtures and clamping mechanisms"
        ]
    },
    # S3: Electronics and Wiring Installation
    'S3': {
        'bottleneck_forecast': [
            "‚ö†Ô∏è Throughput forecast to drop by {decline_pct}% in next {horizon}h",
            "Add {additional_stations} more smart torque stations",
            "Implement real-time torque monitoring with IoT sensors",
            "Reduce changeover time with quick-release tooling"
        ],
        'quality_risk_forecast': [
            "‚ö†Ô∏è Wiring defect rate forecast to increase",
            "Add automated wire continuity testing",
            "Implement torque traceability for all critical connections",
            "Install vision inspection for cable routing accuracy"
        ],
        'maintenance_forecast': [
            "‚ö†Ô∏è Smart torque driver calibration needed soon",
            "Calibrate smart torque drivers within {action_window}h",
            "Replace worn tooling and check sensor connections",
            "Service cable routing guides and strain relief systems"
        ]
    },
    # S4: Automated Calibration and Testing
    'S4': {
        'bottleneck_forecast': [
            "‚ö†Ô∏è Calibration bottleneck forecast - cycle time to reach {forecast_value:.1f}s",
            "Add parallel cable crimping machine (reduce bottleneck by {bottleneck_reduction_pct}%)",
            "Upgrade thermal chamber (ROI: {roi_months} months at ${margin_per_unit}/unit)",
            "Implement predictive maintenance for crimping heads"
        ],
        'quality_risk_forecast': [
            "‚ö†Ô∏è Calibration accuracy forecast to degrade",
            "Upgrade motion testing sensors for higher accuracy",
            "Implement automated calibration data logging",
            "Add redundant measurement systems for critical parameters"
        ],
        'maintenance_forecast': [
            "‚ö†Ô∏è Calibration fixture maintenance due soon",
            "Service calibration fixtures and measurement sensors within {action_window}h",
            "Replace worn crimping heads and check alignment",
            "Calibrate thermal chamber temperature profiles"
        ]
    },
    # S5: Quality Inspection and Finalization
    'S5': {
        'bottleneck_forecast': [
            "‚ö†Ô∏è Inspection throughput forecast to decline by {decline_pct}%",
            "Add one more test fixture (increase capacity by {capacity_increase_pct}%)",
            "Optimize test sequence - parallel testing where possible",
            "Upgrade laser sensors for {sensor_improvement_pct}% faster measurement"
        ],
        'quality_risk_forecast': [
            "‚ö†Ô∏è Inspection accuracy forecast to drop below threshold",
            "Upgrade machine vision resolution and lighting",
            "Add automated defect classification AI",
            "Implement SPC (Statistical Process Control) monitoring"
        ],
        'maintenance_forecast': [
            "‚ö†Ô∏è Vision system maintenance needed soon",
            "Service machine vision cameras and lighting within {action_window}h",
            "Calibrate measurement sensors and test fixtures",
            "Replace worn test probes and connectors"
        ]
    },
    # S6: Packaging and Dispatch
    'S6': {
        'bottleneck_forecast': [
            "‚ö†Ô∏è Dispatch bottleneck forecast - packaging time to increase {growth_pct}%",
            "Add automated palletizing system to reduce packaging time",
            "Upgrade vision system for {vision_improvement_pct}% faster inspection",
            "Implement batch packaging to reduce cycle time"
        ],
        'quality_risk_forecast': [
            "‚ö†Ô∏è Package quality issues forecast to increase",
            "Add automated package weight verification",
            "Implement barcode scanning for shipping accuracy",
            "Install vision inspection for package sealing quality"
        ],
        'maintenance_forecast': [
            "‚ö†Ô∏è Packaging equipment maintenance due soon",
            "Service automated box sealer and taping mechanism within {action_window}h",
            "Replace worn conveyor belts and rollers",
            "Calibrate packaging sensors and vision systems"
        ]
    },
    # Default recommendations
    'default': {
        'bottleneck_forecast': [
            "‚ö†Ô∏è Bottleneck forecasted in next {horizon} hours",
            "Analyze cycle time trends and identify optimization opportunities",
            "Consider adding parallel machines or upgrading equipment",
            "Review buffer sizes and material flow"
        ],
        'quality_risk_forecast': [
            "‚ö†Ô∏è Quality degradation forecasted",
            "Implement additional quality checks and inspections",
            "Review process parameters and adjust as needed",
            "Train operators on quality standards"
        ],
        'maintenance_forecast': [
            "‚ö†Ô∏è Maintenance needed soon based on forecast",
            "Schedule preventive maintenance within {action_window} hours",
            "Inspect critical components and replace worn parts",
            "Calibrate sensors and measurement systems"
        ],
        'normal_forecast': [
            "‚úÖ Production line forecast shows stable operation",
            "Continue monitoring key performance indicators",
            "Maintain preventive maintenance schedule",
            "No immediate action required"
        ]
    }
}


def generate_forecast_recommendation(
    metric_name: str,
    forecast_values: np.ndarray,
    current_value: float,
    station_id: Optional[str],
    row: Dict[str, Any],
    context: Dict[str, Any],
    horizon: int
) -> Tuple[str, List[str], str]:
    """
    Generate intelligent, proactive recommendations based on forecasted trends.
    Returns: (main_recommendation, detailed_actions, alert_level)
    """
    # Calculate trend
    forecast_mean = np.mean(forecast_values)
    forecast_max = np.max(forecast_values)
    forecast_min = np.min(forecast_values)
    trend_pct = ((forecast_mean - current_value) /
                 current_value * 100) if current_value != 0 else 0

    # Determine alert level and issue type
    alert_level = 'normal'
    issue_type = 'normal_forecast'

    # Define thresholds for different metrics
    thresholds = {
        # 10%, 25% increase
        'cycle_time_s': {'warning': 1.1, 'critical': 1.25},
        # 20%, 50% increase
        'queue_len': {'warning': 1.2, 'critical': 1.5},
        'defect_rate': {'warning': 0.03, 'critical': 0.05},  # 3%, 5%
        'failure_rate': {'warning': 0.02, 'critical': 0.04},  # 2%, 4%
        'downtime_sec': {'warning': 120, 'critical': 300},   # 2min, 5min
        # 10%, 25% decrease
        'throughput_per_hour': {'warning': 0.9, 'critical': 0.75},
        'utilization_pct': {'warning': 0.85, 'critical': 0.95},     # 85%, 95%
    }

    threshold_config = thresholds.get(
        metric_name, {'warning': 1.1, 'critical': 1.25})

    if metric_name in ['throughput_per_hour']:
        # For metrics where decrease is bad
        if forecast_mean < current_value * threshold_config.get('critical', 0.75):
            alert_level = 'critical'
            issue_type = 'bottleneck_forecast'
        elif forecast_mean < current_value * threshold_config.get('warning', 0.9):
            alert_level = 'warning'
            issue_type = 'bottleneck_forecast'
    elif metric_name in ['defect_rate', 'failure_rate']:
        # For rate metrics where increase is bad
        threshold_val = threshold_config.get('critical', 0.05)
        if forecast_mean > threshold_val:
            alert_level = 'critical'
            issue_type = 'quality_risk_forecast' if metric_name == 'defect_rate' else 'maintenance_forecast'
        elif forecast_mean > threshold_config.get('warning', threshold_val * 0.6):
            alert_level = 'warning'
            issue_type = 'quality_risk_forecast' if metric_name == 'defect_rate' else 'maintenance_forecast'
    else:
        # For metrics where increase is bad
        if forecast_mean > current_value * threshold_config.get('critical', 1.25):
            alert_level = 'critical'
            issue_type = 'bottleneck_forecast'
        elif forecast_mean > current_value * threshold_config.get('warning', 1.1):
            alert_level = 'warning'
            issue_type = 'bottleneck_forecast'

    # Get station-specific templates or use default
    station_key = station_id if station_id in INTELLIGENT_RECOMMENDATIONS else 'default'
    templates = INTELLIGENT_RECOMMENDATIONS[station_key].get(issue_type,
                                                             INTELLIGENT_RECOMMENDATIONS['default'].get(issue_type, []))

    if not templates:
        return f"Unknown forecast pattern for {metric_name}", [], 'normal'

    # Calculate dynamic values for template substitution
    template_vars = {
        # Basic info
        'station_id': station_id or 'unknown',
        'metric_name': metric_name.replace('_', ' ').title(),
        'current_value': round(current_value, 2),
        'forecast_value': round(forecast_mean, 2),
        'forecast_max': round(forecast_max, 2),
        'forecast_min': round(forecast_min, 2),
        'horizon': horizon,

        # Trend analysis
        'trend_pct': round(abs(trend_pct), 1),
        'growth_pct': round(trend_pct, 1) if trend_pct > 0 else round(-trend_pct, 1),
        'decline_pct': round(-trend_pct, 1) if trend_pct < 0 else 0,

        # Thresholds
        'threshold': threshold_config.get('warning', 0.1),
        'critical_threshold': threshold_config.get('critical', 0.25),

        # Station metrics
        'cycle_time_s': round(row.get('cycle_time_s', 0), 1),
        'current_qty': row.get('parallel_machines', 1),
        'mttr_s': round(row.get('mttr_s', 0)),
        'mtbf_h': round(row.get('mtbf_h', 0)),
        'mtbf_forecast': round(row.get('mtbf_h', 0) * (1 - abs(trend_pct)/200), 1),
        'mtbf_current': round(row.get('mtbf_h', 0), 1),
        'failure_rate': round(row.get('failure_rate', 0) * 100, 2),
        'criticality': row.get('criticality', 'medium'),

        # Calculated improvements
        'improvement_pct': min(30, max(15, int(abs(trend_pct) * 0.7))),
        'capacity_increase_pct': min(40, max(20, int(abs(trend_pct) * 0.8))),
        'bottleneck_reduction_pct': min(40, max(25, int(abs(trend_pct) * 0.9))),

        # Buffer recommendations
        'current_buffer': context.get('buffers', {}).get(f'{station_id}_to_next', 2),
        'buffer_target': min(15, max(8, int(row.get('cycle_time_s', 0) * 0.7) + int(abs(trend_pct)/10))),

        # Equipment scaling
        'additional_machines': max(1, min(2, int(row.get('parallel_machines', 1) * 0.5))),
        'additional_stations': max(1, min(3, int(row.get('parallel_machines', 1) * 0.3))),
        'target_qty': row.get('parallel_machines', 1) + max(1, min(2, int(row.get('parallel_machines', 1) * 0.5))),

        # ROI estimates
        'roi_months': round(6 + row.get('cycle_time_s', 0) * 0.2, 1),
        'margin_per_unit': round(15 + row.get('cycle_time_s', 0) * 0.5, 1),

        # Quality improvements
        'sensor_improvement_pct': min(30, max(15, int(row.get('cycle_time_s', 0) * 1.5))),
        'vision_improvement_pct': min(35, max(20, int(row.get('cycle_time_s', 0) * 1.8))),
        'cycle_reduction_pct': min(20, max(10, int(row.get('cycle_time_s', 0) * 0.8))),
        'reduction_pct': min(25, max(10, int(row.get('setup_time_s', 0) * 0.15))),

        # Action timing
        'action_window': max(4, min(48, int(horizon * 0.5))),
    }

    # Fill in templates with actual values
    detailed_actions = []
    for template in templates[:5]:  # Limit to top 5 recommendations
        try:
            filled_template = template
            for key, value in template_vars.items():
                placeholder = '{' + key + '}'
                if placeholder in filled_template:
                    filled_template = filled_template.replace(
                        placeholder, str(value))
            detailed_actions.append(filled_template)
        except Exception as e:
            detailed_actions.append(template)

    # Create main recommendation summary
    emoji = {'normal': '‚úÖ', 'warning': '‚ö†Ô∏è', 'critical': 'üö®'}[alert_level]
    main_recommendation = f"{emoji} {metric_name.replace('_', ' ').title()}: "

    if alert_level == 'normal':
        main_recommendation += f"Stable forecast (¬±{abs(trend_pct):.1f}%)"
    elif alert_level == 'warning':
        direction = "increasing" if trend_pct > 0 else "decreasing"
        main_recommendation += f"Forecast {direction} by {abs(trend_pct):.1f}% in {horizon}h"
    else:  # critical
        direction = "increasing" if trend_pct > 0 else "decreasing"
        main_recommendation += f"CRITICAL: Forecast {direction} by {abs(trend_pct):.1f}% in {horizon}h"

    if station_id:
        main_recommendation += f" at station {station_id}"

    return main_recommendation, detailed_actions, alert_level

# ==================== DATA PROCESSING FUNCTIONS ====================


def validate_json_extension(filepath: str) -> None:
    """Validate that input file has .json extension."""
    if not filepath.lower().endswith('.json'):
        raise ValueError(
            f"Invalid file format. Only .json files are accepted. "
            f"Received: {filepath}"
        )


def load_json_file(filepath: str) -> pd.DataFrame:
    """Load and normalize JSON data (handles both array and single object)."""
    validate_json_extension(filepath)
    if not os.path.exists(filepath):
        raise FileNotFoundError(f"File not found: {filepath}")
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except json.JSONDecodeError as e:
        raise ValueError(f"Invalid JSON format in {filepath}: {e}")
    # Normalize nested structures
    if isinstance(data, dict):
        data = [data]
    return pd.json_normalize(data, sep='.')


def detect_station_configuration(df: pd.DataFrame) -> bool:
    """Detect if flattened JSON contains station configuration structure."""
    return any(col.startswith('stations.') for col in df.columns) and len(df) == 1


def detect_time_series_data(df: pd.DataFrame) -> bool:
    """Detect if data contains time-series structure."""
    time_cols = ['timestamp', 'time', 'date',
                 'datetime', 'hour', 'simulation_time']
    return any(col in df.columns for col in time_cols) or len(df) > 10


def transform_station_config_to_timeseries(df: pd.DataFrame, context: bool = True) -> Tuple[pd.DataFrame, Dict]:
    """
    Transform nested station configuration into time-series format.
    Simulates historical data for training.
    """
    record = df.iloc[0].to_dict()

    # Extract station keys
    station_prefixes = sorted(set(
        col.split('.')[0] + '.' + col.split('.')[1]
        for col in record.keys() if col.startswith('stations.')
    ))

    # Generate synthetic time-series data for each station
    all_series = []
    for prefix in station_prefixes:
        station_data = {
            k.replace(prefix + '.', ''): v
            for k, v in record.items()
            if k.startswith(prefix + '.')
        }
        station_id = prefix.split('.')[-1].strip()

        # Generate time-series for each target metric
        n_timesteps = 100  # Historical data points
        base_values = {}

        for metric in FORECAST_TARGETS:
            base_val = station_data.get(metric, 0)
            if base_val == 0:
                # Use defaults based on station type
                defaults = {
                    'cycle_time_s': 12.0,
                    'queue_len': 5.0,
                    'defect_rate': 0.02,
                    'failure_rate': 0.015,
                    'downtime_sec': 60.0,
                    'throughput_per_hour': 300.0,
                    'utilization_pct': 0.75
                }
                base_val = defaults.get(metric, 0.0)

            base_values[metric] = base_val

            # Generate time-series with noise and trends
            np.random.seed(abs(hash(station_id + metric)) % 2**32)
            noise = np.random.normal(0, base_val * 0.1, n_timesteps)
            trend = np.linspace(0, base_val * 0.05,
                                n_timesteps)  # Slight trend
            seasonal = base_val * 0.05 * \
                np.sin(np.linspace(0, 4*np.pi, n_timesteps))

            values = base_val + noise + trend + seasonal
            values = np.maximum(values, 0)  # Ensure non-negative

            # Create time-series dataframe
            ts_df = pd.DataFrame({
                'station_id': station_id,
                'metric': metric,
                'timestep': range(n_timesteps),
                'value': values
            })
            all_series.append(ts_df)

    result_df = pd.concat(all_series, ignore_index=True)

    # Extract context for recommendations
    context_dict = {}
    if context:
        buffers = {k.replace('buffers.', ''): v for k,
                   v in record.items() if k.startswith('buffers.')}
        context_dict['buffers'] = buffers
        maintenance = {k.replace('maintenance.', ''): v for k,
                       v in record.items() if k.startswith('maintenance.')}
        context_dict['maintenance'] = maintenance
        hr = {k.replace('human_resources.', ''): v for k,
              v in record.items() if k.startswith('human_resources.')}
        context_dict['human_resources'] = hr

    return result_df, context_dict


def ensure_json_extension(filepath: str) -> str:
    """Ensure output file has .json extension."""
    if not filepath.lower().endswith('.json'):
        base = os.path.splitext(filepath)[0]
        return f"{base}.json"
    return filepath


def ensure_joblib_extension(filepath: str) -> str:
    """Ensure model file has .joblib extension."""
    if not filepath.lower().endswith('.joblib'):
        base = os.path.splitext(filepath)[0]
        return f"{base}.joblib"
    return filepath

# ==================== CHRONOS FORECASTING FUNCTIONS ====================


class ChronosForecaster:
    """Wrapper for Chronos time-series forecasting with fallback."""

    def __init__(self, model_name: str = CHRONOS_MODEL_NAME, device: str = None):
        self.model_name = model_name
        self.device = device or (
            'cuda' if torch.cuda.is_available() else 'cpu')
        self.pipeline = None
        self.use_chronos = False

    def load_model(self):
        """Load Chronos model if available, otherwise use fallback"""
        global CHRONOS_AVAILABLE, CHRONOS_METHOD

        if not CHRONOS_AVAILABLE:
            print("‚ö†Ô∏è  Chronos not available. Using fallback forecasting method.")
            self.use_chronos = False
            return

        print(f"Loading Chronos model: {self.model_name}")
        print(f"Device: {self.device}")
        print(f"Method: {CHRONOS_METHOD}")

        try:
            if CHRONOS_METHOD == 'chronos_package':
                # Use official Chronos package
                self.pipeline = ChronosPipeline.from_pretrained(
                    self.model_name,
                    device_map=self.device,
                    torch_dtype=torch.bfloat16 if self.device != 'cpu' else torch.float32,
                )
                self.use_chronos = True
                print("‚úì Loaded using chronos package")
            elif CHRONOS_METHOD == 'transformers':
                # Use HuggingFace transformers pipeline
                self.pipeline = pipeline(
                    "text2text-generation",
                    model=self.model_name,
                    device=0 if self.device == 'cuda' else -1,
                    torch_dtype=torch.float32
                )
                self.use_chronos = True
                print("‚úì Loaded using transformers pipeline")
        except Exception as e:
            print(f"‚ö†Ô∏è  Failed to load Chronos model: {e}")
            print("   Using fallback forecasting method")
            self.use_chronos = False

    def prepare_time_series(self, values: np.ndarray, context_length: int = 512) -> np.ndarray:
        """Prepare time series by truncating/padding to context length."""
        if len(values) > context_length:
            return values[-context_length:]
        return values

    def forecast(self,
                 series: np.ndarray,
                 forecast_horizon: int = 24,
                 num_samples: int = 20) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """
        Generate forecast using Chronos or fallback method.
        Returns: (median_forecast, quantile_10, quantile_90)
        """
        if self.pipeline is None:
            self.load_model()

        if self.use_chronos and self.pipeline is not None and CHRONOS_METHOD == 'chronos_package':
            try:
                # Prepare series
                series = self.prepare_time_series(series)

                # Convert to tensor
                series_tensor = torch.tensor(
                    series, dtype=torch.float32).unsqueeze(0).to(self.device)

                # Generate forecast
                with torch.no_grad():
                    forecast = self.pipeline.predict(
                        series_tensor,
                        prediction_length=forecast_horizon,
                        num_samples=num_samples,
                    )

                # Convert to numpy and compute quantiles
                forecast_np = forecast.cpu().numpy()
                median = np.median(forecast_np, axis=0).squeeze()
                q10 = np.quantile(forecast_np, 0.1, axis=0).squeeze()
                q90 = np.quantile(forecast_np, 0.9, axis=0).squeeze()

                return median, q10, q90

            except Exception as e:
                print(f"‚ö†Ô∏è  Chronos forecasting failed: {e}")
                print("   Falling back to simple method")

        # Fallback: Simple exponential smoothing with trend detection
        print("   Using exponential smoothing with trend detection")
        last_val = series[-1]

        # Detect trend from recent data
        if len(series) >= 10:
            recent_trend = np.mean(np.diff(series[-10:]))
        else:
            recent_trend = 0

        # Simple forecast with trend
        forecast = np.zeros(forecast_horizon)
        for i in range(forecast_horizon):
            forecast[i] = last_val + (i + 1) * recent_trend

        # Add some uncertainty for quantiles
        noise_std = np.std(series) * \
            0.1 if len(series) > 1 else last_val * 0.05
        median = forecast
        q10 = forecast - noise_std
        q90 = forecast + noise_std

        # Ensure non-negative values
        q10 = np.maximum(q10, 0)
        median = np.maximum(median, 0)
        q90 = np.maximum(q90, 0)

        return median, q10, q90

    def save(self, filepath: str):
        """Save the forecaster."""
        dump({
            'model_name': self.model_name,
            'device': self.device,
            'use_chronos': self.use_chronos
        }, filepath)

    @staticmethod
    def load(filepath: str):
        """Load a saved forecaster."""
        data = load(filepath)
        forecaster = ChronosForecaster(
            model_name=data['model_name'],
            device=data['device']
        )
        forecaster.use_chronos = data['use_chronos']
        return forecaster

# ==================== TRAINING COMMAND ====================


def train_command(args: argparse.Namespace) -> None:
    """Train Chronos forecasting model."""
    print("="*70)
    print("CHRONOS TIME-SERIES FORECASTING MODEL TRAINING")
    print("="*70)

    all_dfs = []
    file_info = []

    # Load and process each JSON file
    for filepath in args.data:
        print(f"\nLoading: {filepath}")
        df = load_json_file(filepath)

        # Detect data type
        is_station_config = detect_station_configuration(df)
        is_time_series = detect_time_series_data(df)

        if is_station_config:
            print(
                f"  ‚Üí Detected station configuration. Generating synthetic time-series...")
            df, _ = transform_station_config_to_timeseries(df, context=False)
            print(f"  ‚Üí Generated {len(df)} time-series samples")
            file_type = 'station_config_synthetic'
        elif is_time_series:
            print(f"  ‚Üí Detected time-series data")
            file_type = 'time_series'
        else:
            print(f"  ‚Üí Warning: Data format not recognized")
            file_type = 'unknown'

        file_info.append({
            'filename': os.path.basename(filepath),
            'type': file_type,
            'rows': len(df)
        })
        all_dfs.append(df)

    # Concatenate all dataframes
    df_combined = pd.concat(all_dfs, ignore_index=True)

    print(f"\n{'='*60}")
    print(
        f"Combined dataset: {len(df_combined)} total rows from {len(args.data)} files")
    print(f"{'='*60}")
    for info in file_info:
        print(
            f"  {info['filename']:<30} {info['type']:<25} {info['rows']:>6} rows")
    print(f"{'='*60}\n")

    # Initialize training report
    training_report = {
        'row_count': len(df_combined),
        'files_processed': file_info,
        'forecast_horizon': args.forecast_horizon,
        'model_name': CHRONOS_MODEL_NAME,
        'chronos_available': CHRONOS_AVAILABLE,
        'chronos_method': CHRONOS_METHOD,
        'timestamp': datetime.now().isoformat()
    }

    # Create and save Chronos forecaster
    forecaster = ChronosForecaster(model_name=CHRONOS_MODEL_NAME)

    # Save model
    model_out = ensure_joblib_extension(args.model_out)
    forecaster.save(model_out)

    # Save training report
    report_out = ensure_json_extension(args.report_out)
    with open(report_out, 'w', encoding='utf-8') as f:
        json.dump(training_report, f, indent=2, default=str)

    print("\n" + "="*70)
    print("TRAINING COMPLETE")
    print("="*70)
    print(f"‚úì Model saved to: {model_out}")
    print(f"‚úì Training report saved to: {report_out}")
    print(f"‚úì Forecast horizon: {args.forecast_horizon} timesteps")
    print(f"‚úì Chronos model: {CHRONOS_MODEL_NAME}")
    print(f"‚úì Chronos available: {CHRONOS_AVAILABLE} ({CHRONOS_METHOD})")
    print("="*70)

# ==================== FORECAST COMMAND ====================


def forecast_command(args: argparse.Namespace) -> None:
    """Generate forecasts and recommendations."""
    print("="*80)
    print("CHRONOS FORECASTING & INTELLIGENT RECOMMENDATIONS")
    print("="*80)

    # Load forecaster
    model_path = args.model_path if args.model_path else DEFAULT_MODEL_PATH
    try:
        forecaster = ChronosForecaster.load(model_path)
        forecaster.load_model()  # Load actual Chronos model if available
    except FileNotFoundError:
        raise ValueError(
            f"Model file not found: {model_path}\n"
            f"Train a model first: python ai_agent_chronos.py train --data file1.json ..."
        )
    except Exception as e:
        raise ValueError(f"Failed to load model from {model_path}: {e}")

    # Load input data
    df = load_json_file(args.input)

    # Detect and transform data
    context = {}
    if detect_station_configuration(df):
        print("\nDetected station configuration JSON. Generating forecasts...")
        df, context = transform_station_config_to_timeseries(df, context=True)
        print(f"Generated forecasts for {df['station_id'].nunique()} stations")
    elif detect_time_series_data(df):
        print("\nDetected time-series data")
    else:
        raise ValueError("Input data format not recognized")

    # Generate forecasts for each station and metric
    stations = df['station_id'].unique()
    all_recommendations = []
    all_forecasts = []

    print("\n" + "="*80)
    print("FORECAST RESULTS")
    print("="*80)

    for station_id in stations:
        station_data = df[df['station_id'] == station_id]
        print(f"\n{'‚îÄ'*78}")
        print(f"STATION {station_id}")
        print(f"{'‚îÄ'*78}")

        station_recommendations = []
        station_forecasts = {
            'station_id': station_id,
            'forecasts': {}
        }

        for metric in FORECAST_TARGETS:
            metric_data = station_data[station_data['metric'] == metric]

            if len(metric_data) == 0:
                continue

            # Get time series values
            values = metric_data['value'].values
            current_value = values[-1]

            # Generate forecast
            median_forecast, q10, q90 = forecaster.forecast(
                values,
                forecast_horizon=args.forecast_horizon,
                num_samples=args.num_samples
            )

            # Store forecast
            station_forecasts['forecasts'][metric] = {
                'current_value': float(current_value),
                'forecast_median': median_forecast.tolist(),
                'forecast_q10': q10.tolist(),
                'forecast_q90': q90.tolist(),
                'forecast_horizon': args.forecast_horizon
            }

            # Generate recommendations
            main_rec, detailed_actions, alert_level = generate_forecast_recommendation(
                metric_name=metric,
                forecast_values=median_forecast,
                current_value=current_value,
                station_id=station_id,
                row=station_data.iloc[0].to_dict(),
                context=context,
                horizon=args.forecast_horizon
            )

            # Create recommendation item
            rec_item = {
                'station_id': station_id,
                'metric': metric,
                'prediction': alert_level,
                'main_recommendation': main_rec,
                'detailed_actions': detailed_actions,
                'current_value': float(current_value),
                'forecast_mean': float(np.mean(median_forecast)),
                'forecast_max': float(np.max(median_forecast)),
                'trend_pct': float(((np.mean(median_forecast) - current_value) / current_value * 100) if current_value != 0 else 0)
            }
            station_recommendations.append(rec_item)

            # Print forecast
            trend_pct = rec_item['trend_pct']
            trend_arrow = "‚Üë" if trend_pct > 0 else "‚Üì" if trend_pct < 0 else "‚Üí"
            print(f"\n{metric.replace('_', ' ').title():<25} {current_value:>8.2f} ‚Üí {np.mean(median_forecast):>8.2f} "
                  f"({trend_arrow} {abs(trend_pct):>5.1f}%) [{alert_level.upper()}]")

            if alert_level != 'normal':
                print(f"  üí° {main_rec}")
                if detailed_actions:
                    # Show top 2
                    for i, action in enumerate(detailed_actions[:2], 1):
                        print(f"     {i}. {action[:80]}...")

        all_recommendations.extend(station_recommendations)
        all_forecasts.append(station_forecasts)

    print("\n" + "="*80)

    # Save recommendations
    rec_output = ensure_json_extension(
        args.output or 'forecast_recommendations.json')
    with open(rec_output, 'w', encoding='utf-8') as f:
        json.dump(all_recommendations, f, indent=2, default=str)

    # Save forecasts
    forecast_output = ensure_json_extension(
        args.forecast_output or 'forecasts.json')
    with open(forecast_output, 'w', encoding='utf-8') as f:
        json.dump(all_forecasts, f, indent=2, default=str)

    # Summary statistics
    critical_count = sum(
        1 for r in all_recommendations if r['prediction'] == 'critical')
    warning_count = sum(
        1 for r in all_recommendations if r['prediction'] == 'warning')

    print("\n" + "="*80)
    print("SUMMARY")
    print("="*80)
    print(f"‚úÖ Forecasts generated for {len(stations)} stations")
    print(f"‚úÖ {len(all_recommendations)} metric forecasts analyzed")
    print(f"üö® Critical alerts: {critical_count}")
    print(f"‚ö†Ô∏è  Warning alerts: {warning_count}")
    print(f"‚úÖ Recommendations saved to: {rec_output}")
    print(f"‚úÖ Forecasts saved to: {forecast_output}")
    print("="*80)

# ==================== MAIN ====================


def main() -> None:
    """Main entry point for CLI."""
    parser = argparse.ArgumentParser(
        description="AI Agent for 6-Station 3D Printer Production Line\n"
                    "‚úì Chronos Time-Series Forecasting\n"
                    "‚úì Predictive, proactive recommendations\n"
                    "‚úì Context-aware with buffer/maintenance data\n"
                    "‚úì Forecasts bottlenecks before they occur",
        formatter_class=argparse.RawTextHelpFormatter
    )
    subparsers = parser.add_subparsers(dest='command', required=True)

    # Train command
    train_parser = subparsers.add_parser(
        'train',
        help='Train Chronos forecasting model',
        description='Train time-series forecasting model using Chronos.\n'
                    'Supports station configuration and time-series data.'
    )
    train_parser.add_argument(
        '--data',
        type=str,
        nargs='+',
        required=True,
        help='One or more JSON files to train on'
    )
    train_parser.add_argument(
        '--model-out',
        type=str,
        default=DEFAULT_MODEL_PATH,
        help=f'Path to save trained model (default: {DEFAULT_MODEL_PATH})'
    )
    train_parser.add_argument(
        '--report-out',
        type=str,
        default='training_report.json',
        help='Path to save training report (default: training_report.json)'
    )
    train_parser.add_argument(
        '--forecast-horizon',
        type=int,
        default=FORECAST_HORIZON,
        help=f'Forecast horizon in timesteps (default: {FORECAST_HORIZON})'
    )

    # Forecast command
    forecast_parser = subparsers.add_parser(
        'forecast',
        help='Generate forecasts and proactive recommendations',
        description='Generate time-series forecasts using Chronos.\n'
                    'Provides proactive recommendations before issues occur.'
    )
    forecast_parser.add_argument(
        'model_path',
        type=str,
        nargs='?',
        default=None,
        help=f'Path to trained model (default: {DEFAULT_MODEL_PATH})'
    )
    forecast_parser.add_argument(
        '--input',
        type=str,
        required=True,
        help='Path to input data JSON file'
    )
    forecast_parser.add_argument(
        '--output',
        type=str,
        default=None,
        help='Path to save recommendations (default: forecast_recommendations.json)'
    )
    forecast_parser.add_argument(
        '--forecast-output',
        type=str,
        default=None,
        help='Path to save raw forecasts (default: forecasts.json)'
    )
    forecast_parser.add_argument(
        '--forecast-horizon',
        type=int,
        default=FORECAST_HORIZON,
        help=f'Forecast horizon in timesteps (default: {FORECAST_HORIZON})'
    )
    forecast_parser.add_argument(
        '--num-samples',
        type=int,
        default=20,
        help='Number of samples for probabilistic forecasting (default: 20)'
    )

    args = parser.parse_args()

    try:
        if args.command == 'train':
            train_command(args)
        elif args.command == 'forecast':
            forecast_command(args)
    except Exception as e:
        print(f"\n‚ùå Error: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()
